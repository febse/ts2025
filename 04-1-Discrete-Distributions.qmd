```{r, output=FALSE, warning=FALSE}
library(tidyverse)
```

# Discrete Distributions (Review)

Let $X$ and $Y$ be two discrete random variables with possible values $0, 1, \ldots, K$. The probability mass functions $p_X(x)$ and $p_Y(y)$ assign occurrence probabilities to each possible outcome.

For example:

```{r, echo=FALSE, warning=FALSE, output=FALSE}
px <- read_csv("https://raw.githubusercontent.com/febse/data/main/econ/prob_review/px.csv")
py <- read_csv("https://raw.githubusercontent.com/febse/data/main/econ/prob_review/py.csv")
pxy <- read_csv("https://raw.githubusercontent.com/febse/data/main/econ/prob_review/pxy.csv") %>%
  select(x, y, p)
```

$$
p_X(x) = \begin{cases}
0.250 & \text{for } x = 0\\
0.095 & \text{for } x = 1 \\
0.272 & \text{for } x = 2 \\
0.383 & \text{for } x = 3 \\
0 & \text{otherwise}
\end{cases}
$$

$$
p_Y(y) = \begin{cases}
0.76 & \text{for } y = 2\\
0.24 & \text{for } y = 3 \\
0 & \text{otherwise}
\end{cases}
$$

For convenience, the probability mass functions of $X$ and $Y$ are stored in the tibbles `px` and `py`.

```{r}
#| label: tbl-distr-x
#| tbl-cap: "Probability mass function of $X$."
 
px %>%
  knitr::kable()
```

```{r}
#| label: tbl-distr-y
#| tbl-cap: "Probability mass function of $Y$"
 
py %>%
  knitr::kable()
```

The usual visualization of a probability mass function is a bar plot.

```{r}
px %>%
  ggplot(aes(x = x, y = p)) + 
    geom_col() + 
    labs(
      x = "Outcome",
      y = "Probability"
    )
```

The probability functions sum to one over all possible values by definition:

$$
\sum_{x = 0}^{3}p_X(x) = 0.250 + 0.095 + 0.272 + 0.383 = 1
$$

::: {#exm-sampling-x}
## Sampling from a uni-variate distribution

```{r}
set.seed(12)

smpl_x <- px %>%
  slice_sample(n = 20000, weight_by = p, replace = TRUE)
  
smpl_x_summary <- smpl_x %>%
  group_by(x) %>%
  summarize(
    n = n(),
    ObservedShare = n / nrow(smpl_x),
    Probability = first(p)
  )
smpl_x_summary
```

It is useful to visualize the observed frequency of each outcome.

```{r}
smpl_x_summary %>%
  select(x, ObservedShare, Probability) %>%
  pivot_longer(cols = c(ObservedShare, Probability), names_to="type", values_to="value") %>%
  ggplot(aes(x = x, y = value, fill = type)) +
    geom_col(position = "dodge") +
    labs(
      x = "Outcome",
      y = "Share",
      fill = "Type"
    )
```
:::

::: {#exr-prob-y-sample}
## Sampling from the Distribution of Y

Select 2000 samples from the distribution of $Y$ and visualize the frequencies.

```{r}
# Take the samples here

```

```{r}
# Visualize the frequencies here

```
:::

## The Expected Value

The *expected value* of a random variable is the average of all possible values that can occurr, weighted by their occurrence probabilities. It is a measure of the *location* of the distribution.

$$
\begin{align}
\mu_x & = E(X) = \sum_{x = 0}^{3} x p_X(x) = 0 \times 0.250 + 1 \times 0.095 + 2 \times 0.272 + 3 \times 0.383 = 1.788 \\
\end{align}
$$

```{r}
mu_x <- sum(px$x * px$p)
mu_x

mean(smpl_x$x)
```

```{r}
px %>%
  ggplot(aes(x = x, y = p)) + 
    geom_col() + 
    labs(
      x = "Outcome",
      y = "Probability"
    ) + 
    geom_vline(xintercept = mu_x, color = "firebrick")
```

::: {#exr-expectation}
## Expected Value

Compute the expected value of $Y$.
:::

::: solution
```{r}
# Type your code here

```
:::

If you want to predict future values of a random variable, the expected value is your best guess in the sense that it minimizes the expected value of the quadratic loss function:

$$
E[(X - \hat{x})^2]
$$

Let us construct an example. You need to predict the result of $X$ and you think that the best prediction is $\bar{x} = 1$. When the game runs it will produce four possible values: 0, 1, 2, and 3. The error that you will make is:

$$
L(x) = (x - 1)^2 =
\begin{cases}
   (0 - 1)^2 = 1 & \text{x = 0}\\
   (1 - 1)^2 = 0 & \text{x = 1}\\
   (2 - 1)^2 = 1 & \text{x = 2}\\
   (3 - 1)^2 = 4 & \text{x = 3}
\end{cases}
$$

::: {#exr-expected-loss}
## Expected Quadratic Loss

Compute the expected quadratic loss for a prediction $\bar{x} = 1.5$.
:::

::: solution
```{r}
## Type your code here

# px %>%
#   summarise(
#     loss = ?
#   )
```
:::

## Variance

The variance of a random variable (distribution) measures how different the possible values that can occur are. Values that occur more often (have higher probability) under $p_X$ receive a higher weight. Values that occur less frequently under $p_X$ are given a lower weight in the sum.

$$
\begin{align}
\text{Var}(X) & = \sum_{x = 0}^{3} (x - \mu_x)^2 \times p_X(x) \\
       & = (0 - 1.788)^2 \times 0.250 + (1 - 1.788)^2 \times 0.095 + (2 - 1.788)^2\times 0.272 + (3 - 1.788)^2 \times 0.383 \\
       & = (-1.788)^2 \times 0.250 + (-0.788)^2 \times 0.095 + (0.212)^2\times 0.272 + (1.212)^2 \times 0.383 \\
       & = 3.196 \times 0.250 + 0.620^2 \times 0.095 + 0.044 \times 0.272 + 1.468 \times 0.383 \\
       & \approx 1.433
\end{align}
$$ {#eq-variance}

::: {#exr-variance}
## Variance

Compute the Variance of $X$.
:::

::: solution
```{r}
# Type your code here

```
:::

You can see from @eq-variance that it is the expected value of the squared deviations from the expected value.

$$
Var(X) = E(X - E(X))^2
$$ ::: {#def-variance} \## Variance

The variance of a random variable (distribution) is a summary of the distribution and describes its spread: how different are the values that this distribution will generate.

$$
Var(X) = E[(X - E(X))^2] = E(X^2) - E(X)^2
$$ :::

::: {#exr-variance-decomp}
## Variance

Compute the variance of $X$ using the decomposition formula.
:::

::: solution
```{r}
# Type your code here

```
:::

## The Empirical Variance

The (uncorrected) empirical variance is a measure of the spread of the observed values, it is computed as:

$$
\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$ is exactly the same as the variance of a distribution, but instead of the probabilities we have the observed proportions.

For example:

```{r}
x <- c(0, 0, 1, 1, 1, 2, 3, 3, 3)
(x - mean(x))^2
sum((x - mean(x))^2) / length(x)
2.4197531 * 2 / 9 + 0.3086420 * 3 / 9 + 0.1975309 * 1 / 9 + 2.0864198 * 3 / 9
```

If you use the `var` function in R, it will return a slightly different value.

```{r}
var(x)
```

This is because the `var` function in R uses the corrected formula for the variance:

$$
S^2_{x} = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$

```{r}
# Compute both the corrected and the uncorrected variance 
# from the samples of x and compare it with the variance of the distribution


```

## Joint Distribution

The joint probability mass function tells you the probability of the *simultaneous* occurrence of $x$ and $y$. For example, you can ask it the question: what is the probability of $x = 2$ *and* $y = 3$.

For two discrete variables, it is convenient to present the joint distribution as a table with cell entries holding the probabilities. The joint distribution is given in the tibble `pxy` in a long format.

```{r}
pxy %>%
  knitr::kable()
```

Sometimes it is more convenient to see this distribution in a wide format:

```{r}
#| label: tbl-distr-dep
#| tbl-cap: "Joint distribution of $X$ and $Y$."

pxy %>%
  pivot_wider(
    id_cols = x,
    names_from = y,
    values_from = p,
    names_prefix="y="
    ) %>%
  knitr::kable(digits = 3)
```

$$
p_{XY}(x=2, y=3) = 0.043
$$

The joint probability distribution function must sum (integrate) to one over all possible *pairs* of $x$ and $y$.

$$
\sum_{x = 0}^{3}\sum_{y = 2}^{3} p_{XY}(x, y) = 1
$$

```{r}
sum(pxy$p)
```

In the example until now we have summarized the *marginal* distributions of $X$ and $Y$ but we have said nothing about their joint distribution. Usually the joint distribution is determined by the subject matter at hand, but for the sake of example we will look at two joint distributions so that we can get an idea how they work.

First we will construct a special joint distribution under the assumption of *independence*. Intuitively, two random variables are independent, if the outcome of one of the variables does not influence the probability distribution of the other. Imagine that you hold two lottery tickets: one from a lottery in Germany and another from a lottery in Bulgaria. It would be safe to assume that the realized winnings from the German lottery will not affect the odds to win from the Bulgarian ticket.

Now let us consider a case of *dependent* random variables. Let $X$ be the level of a river (at some measurement point) at time $t$ and $Y$ be the level of the same river five minutes later. It would be safe to assume that if the level of the river was high at $t$ this would affect the distribution of the level of the river at $t$ plus five minutes.

## Marginal Distributions {#sec-marginal-distr}

The marginal distribution of $X$ is obtained by summing the joint distribution of $X$ and $Y$ over all possible values of $Y$.

$$
p_X(x) = \sum_{y=2}^{3}p_{XY}(x, y)
$$

```{r}
pxy %>% 
  group_by(x) %>% 
  summarise(p_x = sum(p))
```

::: {#exr-marginal-distr-y}
## Marginal distribution of $Y$

Compute the marginal distribution of $Y$ from the joint distribution in @tbl-distr-dep. Use the `pxy` tibble and the `group_by` and `summarise` functions.

```{r}
# pxy %>%
#   group_by(...) %>%
#   summarise(p_y = sum(p))
```
:::

## Conditional Distributions

```{r}
#| label: tbl-cond-distr-y-dep-x
#| tbl-cap: "Conditional distributions of $Y$ given $X$"

pxy_w <- pxy %>%
  pivot_wider(
    id_cols = x,
    names_from = y,
    values_from = p,
    names_prefix = "y="
  ) %>%
  mutate(
    p_x = `y=2` + `y=3`,
    `y=2` = `y=2` / p_x,
    `y=3` = `y=3` / p_x
  )

pxy_w %>%
  knitr::kable(digits = 3)
```

Looking at the conditional distributions of $Y$ given $X$ in @tbl-cond-distr-y-dep-x, you should notice that these are not the same for each value of $X$. For example, $Y=2$ is much more likely when $X = 0$ compared to $X = 3$.

## Joint Distribution under Independence

Lets construct the joint distribution $p_{XY}(x, y)$ that assigns a probability to the points $(x, y)$, assuming that $X$ and $Y$ are independent.

For independent random variables the joint probability of occurrence is simply the product of the *marginal* distributions.

$$
p_{XY}(x, y) = p_X(x)p_Y(y)
$$

```{r}
pxy_ind <- expand_grid(
  px %>% rename(p_x = p), 
  py %>% rename(p_y = p)
)
pxy_ind <- pxy_ind %>%
  mutate(
    p = p_x * p_y
  )
```

```{r, collapse=TRUE}
#| label: tbl-distr-indep
#| tbl-cap: "Joint distribution of $X$ and $Y$ under independence."

pxy_ind_w <- pxy_ind %>%
  pivot_wider(
    id_cols = x,
    names_from = y,
    values_from = p,
    names_prefix = "y="
  )

pxy_ind_w %>%
  knitr::kable(digits = 3)
```

Let's look at the *conditional* distributions of $Y$ given $X$. These answer the questions of the type: if $X$ turns out to be $0$, what are the probabilities for $Y = 2$ and $Y = 3$.

To get the conditional distributions of Y for each possible value of $X$ we divide the cells of the joint distribution table by the marginal probabilities of each $x$.

$$
p_{Y|X}(x, y) = \frac{p_{XY}(x, y)}{p_X(x)}
$$

```{r}
#| label: tbl-xy-distr-ind-w
#| tbl-cap: "Conditional distributions of $Y$. Independence case."

pxy_ind_w %>%
  mutate(
    p_x = `y=2` + `y=3`,
    `y=2` = `y=2` / p_x,
    `y=3` = `y=3` / p_x
  ) %>%
  knitr::kable(digits = 3)
```

It is convenient to visualize the conditional distributions of $Y$ given $X$.

```{r}
pxy %>%
  ggplot(aes(y = factor(x), x = p, color=factor(y))) +
  geom_col(position="dodge") +
  labs(
    x = "p(x | y)",
    y = "x",
  ) + 
  theme_minimal()
```

What you should see in @tbl-xy-distr-ind-w is that the conditional distributions of $Y$ are the same for every possible value of $X$. This is of course a consequence of the way we constructed this joint distribution in the first place: namely, we assumed that $X$ and $Y$ are independent.

## Conditional Expectation

We have seen how we derived the conditional distributions of $Y$ given $X$ in the previous section. Now we can ask the question: what is the expected value of $Y$ given that $X$ has already turned out to be 0 (for example). We can take the conditional distribution of $Y$ given $X = 0$ and compute the expected value of this distribution.

For the joint distribution under independence:

$$
E(Y | X=0) = \sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \times 0.76 + 3 \times 0.24 = 2.24
$$

```{r}
2 * 0.76 + 3 * 0.24
```

For the joint distribution in @tbl-cond-distr-y-dep-x the conditional expectation of $Y$ given $X = 0$ is

$$
E(Y | X=0) = \sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \times 0.964 + 3 \times 0.036 = 2.036
$$

```{r}
2 * 0.964 + 3 * 0.036
```

Let us write the conditional expectation of $Y$ for each possible value of $X$ for the dependent joint distribution case.

$$
E(Y | X = x) = \begin{cases}
  2.036 & \text{for } x = 0 \\
  2.060 & \text{for } x = 1 \\
  2.158 & \text{for } x = 2 \\
  2.475 & \text{for } x = 3
\end{cases}
$$

```{r}
#| label: tbl-cond-exp-y
#| tbl-cap: "Conditional expectation of $Y$ for each possible value of $X$."

pxy %>%
  group_by(x) %>%
  mutate(
    p_y_x = p / sum(p)
  ) %>%
  summarize(
    E_Y_given_X = sum(y * p_y_x) 
  ) %>%
  knitr::kable(digits = 3)
```

An important thing to see here is that the conditional expectation is different for each value of $X$. As the value of $X$ is uncertain (it is a random variable), the conditional expectation of $Y$ given $X$ is also a random variable. Its distribution is given by the possible values and the probabilities of occurrence of $X$ (the marginal distribution of $X$).

::: {#exr-cond-exp-ind}
Calculate the expected value of $Y$ given $X$ for every possible value of $X$ in the case joint distribution under independence.
:::

::: {#exm-sampling-joint}
## Sampling from the Joint Distribution

```{r}
sample_joint <- pxy %>%
  slice_sample(n = 1000, weight_by = p, replace = TRUE)

head(sample_joint)
```

```{r}
sample_joint %>%
  group_by(x, y) %>%
  summarise(
    p = first(p),
    n = n(),
    f = n / nrow(sample_joint)
  )
```
:::

## Covariance

The *covariance* measures the (linear) dependency between two random variables.

::: {#def-covariance}
## Covariance

The covariance of two random variables $X$ and $Y$ is given by

$$
Cov(X, Y) = E[(X - E(X))(Y - E(Y))]
$$ Alternatively, it can be computed using the decomposition formula:

$$
Cov(X, Y) = E(XY) - E(X)E(Y)
$$
:::

In the analysis of time series we will often encounter situations where the expected value of one of the random variables is zero. As can be seen from the decomposition formula, in that case the covariance reduces to

$$
Cov(X, Y) = E(XY).
$$

Closely related to the covariance is the correlation between $X$ and $Y$.

::: {#def-correlation}
## Correlation

$$
\rho(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}
$$ It is easy to show that the correlation is bounded between -1 and 1.

$$
-1 \leq \rho(X, Y) \leq 1
$$
:::

::: {#exr-corr-proof}
## Correlation

Let X be a random variable with, and $Y = a + bX$. Show that the correlation between $X$ and $Y$ equals one or minus one depending on the sign of $b$. For simplicity, assume that $E(X) = 0$.
:::

::: {#thm-covariance-props}
## Properties of the Covariance

Let $X$ and $Y$ be random variables and let $a, b \in \mathbb{R}$ be fixed constants.

$$
Var(aX + bY) = a^2 Var(X) + b^2Var(Y) + 2abCov(X, Y)
$$
:::

::: {#exr-covariance}
## Covariance

Compute the covariance of $X$ and $Y$ under the joint distributions given in @tbl-distr-indep and @tbl-distr-dep. Use the `pxy` and `pxy_ind` tables for these calculations.
:::

::: solution
```{r}
# Type your code here

```
:::

::: {.callout-note collapse="true"}
## Proof of the Properties of the Expectation

1.  $E(a) = a$

The expected value of a discrete variable $X$ with possible outcomes $x_1, x_2, \ldots, x_n$ and probabilities $p_1, p_2, \ldots, p_n$ is given by

$$
E(X) = \sum_{i = 1}^{n} x_i p_i
$$

Multiplying both sides by $a$ gives

$$
aE(X) = a\sum_{i = 1}^{n} x_i p_i = \sum_{i = 1}^{n} ax_i p_i
$$

The right-hand side is the expected value of a random variable that takes the values $ax_1, ax_2, \ldots, ax_n$ with probabilities $p_1, p_2, \ldots, p_n$. Therefore, $E(aX) = aE(X)$.

2.  $E(X + Y) = E(X) + E(Y)$

This proof involves the joint distribution of $X$ and $Y$ which we will introduce later. Let $p_{XY}(x, y)$ be the joint probability mass function of $X$ and $Y$. The expected value of the sum of two random variables is given by

$$
E(X + Y) = \sum_{x}\sum_{y} (x + y)p_{XY}(x, y)
$$

Expanding the sum gives

$$
\begin{align*}
\sum_{x}\sum_{y} (x + y)p_{XY}(x, y) & = \sum_{x}\sum_{y} xp_{XY}(x, y) + \sum_{x}\sum_{y} yp_{XY}(x, y)
\end{align*}
$$

On the left hand side of the above equation we have two sums. In the first one, we can write $x$ in front of the second sum, because it does not depend on $y$. We can do the same for the second sum where $y$ does not depend on $x$.

$$
\begin{align*}
\sum_{x}\sum_{y} (x + y)p_{XY}(x, y) & = \sum_{x}x\sum_{y} p_{XY}(x, y) + \sum_{y}y\sum_{x} p_{XY}(x, y)
\end{align*}
$$

Now we need to see that summing the joint probability mass function over all possible values of $y$ gives the marginal distribution of $X$ and summing over all possible values of $x$ gives the marginal distribution of $Y$ (see @sec-marginal-distr).

The first sum is the expected value of $X$ and the second sum is the expected value of $Y$. Therefore, $E(X + Y) = E(X) + E(Y)$.
:::

::: {#thm-variance-short}
## Properties of the Variance

Let $X$ be a random variable with expected value $E(X)$, let $Y$ be a random variable with expected value $E(Y)$, and let $a$ be a fixed constant ($a \in \mathbb{R}$). The following properties are true:

$$
Var(X) = E(X^2) - E(X)^2
$$

$$
\begin{align}
Var(a) & = 0 \\
Var(aX) & = a^2 Var(X)
\end{align}
$$

Furthermore, if $X$ and $Y$ are *uncorrelated*, then the variance of their sum equals the sum of their variances:

$$
Var(X + Y) = Var(X) + Var(Y)
$$
:::

::: {#exr-sum}
## Expected value and variance

Use the distributions of $X$ and $Y$ from @tbl-distr-x and @tbl-distr-y to compute the expected value and the variance of

$$
2X + 3Y + 1.
$$

Assume that $X$ and $Y$ are independent.
:::

::: solution
$$
E(2X + 3Y + 1) = \\
Var(2X + 3Y + 1) = 
$$
:::

::: {#thm-exp-value-props}
## Properties of the Expected Value

Let $X$ be a random variable with expected value $E(X)$, let $Y$ be a random variable with expected value $E(Y)$, and let $a$ be a fixed constant ($a \in \mathbb{R}$). The following properties are true:

$$
\begin{align}
E(a) & = a \\
E(aX) & = aE(X) \\
E(X + Y) & = E(X) + E(Y)
\end{align}
$$

Furthermore, if $X$ and $Y$ are *uncorrelated*, then the expected value of the product of the two random variables equals the product of their expected values:

$$
E(XY) = E(X)E(Y)
$$
:::